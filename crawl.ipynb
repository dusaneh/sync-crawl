{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: \n",
    "# - add 'clear popup' actions\n",
    "# - add clear field text action \n",
    "# - expand the window to longer format by default (maybe skip chunking?)\n",
    "\n",
    "# python3 -m venv .venv\n",
    "# source .venv/bin/activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running FastAPI version: 0.115.5\n",
      "Running Pydantic version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import pydantic\n",
    "import fastapi\n",
    "\n",
    "print(f\"Running FastAPI version: {fastapi.__version__}\")\n",
    "print(f\"Running Pydantic version: {pydantic.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start the playwright session:\n",
    "# \n",
    "# # PC\n",
    "## \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" --remote-debugging-port=9223 --user-data-dir=\"C:\\my-chrome-profile\"\n",
    "\n",
    "# mac\n",
    "# /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9223 --user-data-dir=\"~/chrome-debug-profile\"\n",
    "\n",
    "\n",
    "###################################\n",
    "\n",
    "# to start the server which will communicate with the playwright session:\n",
    "## cd C:/Users/dusan/OneDrive/Desktop/pp/test/crawl/\n",
    "## .\\.venv\\Scripts\\activate\n",
    "## cd v14\n",
    "## python fastAPIServ.py\n",
    "\n",
    "## cd ~/Desktop/pp/sync-crawl\n",
    "\n",
    "## git fetch origin\n",
    "## git merge origin/main\n",
    "## git add .\n",
    "## git commit -m \"message\"\n",
    "## git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-05 14:58:12,484 - helper - <module> - Line: 1052 - INFO - Running on Windows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import helper as hlp\n",
    "import get_2fa as g2\n",
    "\n",
    "import hlogger as hlog\n",
    "import os\n",
    "from log_config import get_logger\n",
    "\n",
    "logger = get_logger(\"Jupyter Notebook\")  # Use module name for easier identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-05 14:59:19,347 - Jupyter Notebook - <module> - Line: 28 - INFO - Starting workflow add_cust_v98 for intuit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "client = 'intuit'\n",
    "wd = os.getcwd()  # Gets the current working directory\n",
    "client_folder = 'clients/'+client\n",
    "# --- Configuration ---\n",
    "site_description = \"Quickbooks Online Website\"\n",
    "site_wide_instructions = \"\"\"\n",
    "\"\"\"\n",
    "workflow_id = \"add_cust_v98\" #unique identifier for the workflow\n",
    "# workflow_instructions = \"\"\"Add a new non-inventory type product by navigating to sales->products->New->non-inventory type. Fill out the form with mock data and save the form. Make sure to interact with all form elements to demonstrate and catalog the functionality of the site's function for this workflow.\n",
    "# Non-inventory type product is a products you buy and/or sell but don’t need to (or can’t) track quantities of, for example, nuts and bolts used in an installation.\n",
    "# \"\"\" #instructions for the workflow\n",
    "workflow_instructions = \"\"\"To add a new customer in QuickBooks Online, go to the Sales tab, then select Customers, and click New customer. Fill out the customer’s details in the form and hit Save to finish.\n",
    "\"\"\" #instructions for the workflow\n",
    "# workflow_instructions = \"Add a new customer manually. Follow the icons at the top of the Customer entry form to enter different sections of customer info before saving the form. Enter just basic example data.\" #instructions for the workflow\n",
    "#workflow_instructions = \"Add sleep mask to cart then go to cart and remove it\" #instructions for the workflow\n",
    "\n",
    "client_url = 'https://sandbox.qbo.intuit.com/app/homepage'\n",
    "hlp.navigate_to_url_from_metadata(client_url)\n",
    "\n",
    "extra_instructions = \"\"\n",
    "logger_struct = hlog.HierarchicalLogger(client_folder = client_folder)\n",
    "logger_struct.set_workflow(workflow_id)\n",
    "logger_struct.set_workflow_value(\"site_wide_instructions\", site_wide_instructions)\n",
    "logger_struct.set_workflow_value(\"site_description\", site_description)\n",
    "logger_struct.set_workflow_value(\"workflow_instructions\", workflow_instructions)\n",
    "logger_struct.set_workflow_value(\"client_url\", client_url)\n",
    "\n",
    "logger.info(f\"Starting workflow {workflow_id} for {client}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import requests\n",
    "\n",
    "def fold_screenshot_test(starting_width,starting_height,depth_chunks_to_test,run_rerun_path,workflow_instructions,site_wide_instructions,site_description,directory):\n",
    "\n",
    "    #run_rerun_path = 'clients/qb\\\\cust3\\\\0\\\\0\\\\0\\\\0'\n",
    "    logger.info(f\"Requesting extract_metadata for {client} with starting width {starting_width} and starting height {starting_height}\")\n",
    "    # scans through the page and extracts metadata\n",
    "    metadata_response = requests.get(f\"http://127.0.0.1:8000/extract_metadata?steps={10}&wait_per_step_ms={1000}&overlap_percent=30\",)\n",
    "    if metadata_response.status_code != 200:\n",
    "        logger.error(f\"Failed to fetch metadata. Status code: {metadata_response.status_code}. Message: {metadata_response}\")\n",
    "        raise Exception(f\"Failed to fetch metadata. Status code: {metadata_response.status_code}. Message: {metadata_response}\")\n",
    "        \n",
    "    url_metadata = metadata_response.json()\n",
    "    url_metadata\n",
    "\n",
    "\n",
    "\n",
    "    # modified_height,tokens = hlp.largest_height_given_width(max_tokens=1600,orig_width=starting_width,suggested_height=starting_height)\n",
    "\n",
    "    import copy\n",
    "    new_height = copy.deepcopy(starting_width)\n",
    "\n",
    "    full_height = url_metadata['dimensions']['fullPage']['height']\n",
    "\n",
    "    diff = full_height - new_height\n",
    "    increment = math.ceil(diff/depth_chunks_to_test)\n",
    "\n",
    "\n",
    "    fold_tests = []\n",
    "\n",
    "    i = 1\n",
    "    while i <= depth_chunks_to_test:\n",
    "        logger.info(f\"Resizing browser to width {starting_width} and height {new_height}\")\n",
    "        resize_browser_result = hlp.resize_browser_window(width=starting_width, height=new_height)\n",
    "        fold_tests.append({'width':starting_width,'height':new_height,'id':i})\n",
    "\n",
    "        logger.info(f\"Requesting screenshot for chunk {i}\")\n",
    "        screenshot_response = requests.get(\n",
    "            f\"http://127.0.0.1:8000/screenshot\",## todo\n",
    "            params={\"output_path\": run_rerun_path+f\"/{directory}\", \"overlap_percentage\": 0,\"max_chunks\": 1, \n",
    "                        \"action_id\": None,\n",
    "                        \"candidate_id\": None,\n",
    "                        \"single_chunk_override_id\":i\n",
    "                    }\n",
    "        )\n",
    "        logger.info(f\"Completed screenshot for chunk {i}\")\n",
    "        new_height = new_height + increment\n",
    "        i += 1\n",
    "    resize_browser_result = hlp.resize_browser_window(width=starting_width, height=starting_height)\n",
    "\n",
    "    fold_test_paths = []\n",
    "    #chunk_file_list =  hlp.get_files_with_extension(run_rerun_path+\"/chunks\", \".png\")\n",
    "    for c in fold_tests:\n",
    "        logger.info(f\"Resizing screenshot {c['id']}\")\n",
    "        tmp_resize_screenshot_result = hlp.resize_and_crop(\n",
    "            input_path  = f\"{run_rerun_path}/{directory}/chunk_{c['id']}.png\",\n",
    "            output_path = f\"{run_rerun_path}/{directory}/resized_chunk_{c['id']}.png\"\n",
    "        )\n",
    "        c['path'] = run_rerun_path+f\"/{directory}/resized_chunk_{c['id']}.png\"\n",
    "        c['file_name'] = f\"resized_chunk_{c['id']}.png\"\n",
    "        fold_test_paths.append(c['path'])\n",
    "        tmp_resize_screenshot_result['id'] = c['id']\n",
    "        c['new_dimensions'] = {'height':tmp_resize_screenshot_result['new_height'],'width':tmp_resize_screenshot_result['new_width']}\n",
    "        #print(c['id'],\":\",'width:',c['new_dimensions'])\n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "    return fold_tests,fold_test_paths,screenshot_response,url_metadata,resize_browser_result\n",
    "\n",
    "def perform_fold_test_llm(fold_tests,fold_test_paths,screenshot_response,resize_browser_result):\n",
    "\n",
    "    page_action_json_ranked_fold_test = None\n",
    "\n",
    "    if len(fold_tests) > 1:\n",
    "        logger.info(f\"Requesting fold test llm process because there are {len(fold_tests)} fold tests\")\n",
    "\n",
    "        page_action_json_ranked_fold_test,action_prompt,raw = hlp.perform_llm_fold_test(fold_tests,fold_test_paths,workflow_instructions,site_wide_instructions,site_description)\n",
    "\n",
    "        best_image={}\n",
    "        best_image = max(\n",
    "            page_action_json_ranked_fold_test,\n",
    "            key=lambda x: (x['relevant_elements'], x['clarity']),\n",
    "        )\n",
    "        for f in fold_tests:\n",
    "            if f['id'] == best_image['id']:\n",
    "                best_image['path'] = f['path']\n",
    "                best_image['file_name'] = f['file_name']\n",
    "                best_image['resized_dimensions'] = f['new_dimensions']\n",
    "                best_image['original_dimensions'] = {'height':f['height'],'width':f['width']}\n",
    "        \n",
    "        logger.info(f\"Resizing window for best image is {best_image['file_name']} with clarity {best_image['clarity']} and {best_image['relevant_elements']} relevant elements\")\n",
    "        \n",
    "        resize_browser_result = hlp.resize_browser_window(width=best_image['original_dimensions']['width'], height=best_image['original_dimensions']['height'])\n",
    "\n",
    "    else:\n",
    "        best_image={}\n",
    "        best_image['id'] = fold_tests[0]['id']\n",
    "        best_image['path'] = fold_tests[0]['path']\n",
    "        best_image['file_name'] = fold_tests[0]['file_name']\n",
    "        best_image['resized_dimensions'] = fold_tests[0]['new_dimensions']\n",
    "        best_image['original_dimensions'] = {'height':fold_tests[0]['height'],'width':fold_tests[0]['width']}\n",
    "\n",
    "    #best_image\n",
    "\n",
    "    # resize browse to the best fold\n",
    "\n",
    "    screenshot_data = screenshot_response.json()\n",
    "\n",
    "    return page_action_json_ranked_fold_test,best_image,screenshot_data,resize_browser_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-05 14:59:26,895 - Jupyter Notebook - <module> - Line: 69 - INFO - Previous run_rerun_path is  and current run_rerun_path is clients/intuit\\add_cust_v98\\0\\0\\0\\0\n",
      "2025-04-05 14:59:26,896 - Jupyter Notebook - <module> - Line: 71 - INFO - Starting run 0 rerun 0 run_rerun 0 in clients/intuit\\add_cust_v98\\0\\0\\0\\0\n",
      "2025-04-05 14:59:26,897 - Jupyter Notebook - <module> - Line: 86 - INFO - It's the first run for the sample. Starting fold screenshot test\n",
      "2025-04-05 14:59:26,898 - Jupyter Notebook - fold_screenshot_test - Line: 7 - INFO - Requesting extract_metadata for intuit with starting width 1256 and starting height 1369\n",
      "2025-04-05 14:59:27,975 - Jupyter Notebook - fold_screenshot_test - Line: 34 - INFO - Resizing browser to width 1256 and height 1256\n",
      "2025-04-05 14:59:28,694 - Jupyter Notebook - fold_screenshot_test - Line: 38 - INFO - Requesting screenshot for chunk 1\n",
      "2025-04-05 14:59:28,960 - Jupyter Notebook - fold_screenshot_test - Line: 47 - INFO - Completed screenshot for chunk 1\n",
      "2025-04-05 14:59:28,961 - Jupyter Notebook - fold_screenshot_test - Line: 34 - INFO - Resizing browser to width 1256 and height 1276\n",
      "2025-04-05 14:59:29,020 - Jupyter Notebook - fold_screenshot_test - Line: 38 - INFO - Requesting screenshot for chunk 2\n",
      "2025-04-05 14:59:29,261 - Jupyter Notebook - fold_screenshot_test - Line: 47 - INFO - Completed screenshot for chunk 2\n",
      "2025-04-05 14:59:29,344 - Jupyter Notebook - fold_screenshot_test - Line: 55 - INFO - Resizing screenshot 1\n",
      "2025-04-05 14:59:29,517 - Jupyter Notebook - fold_screenshot_test - Line: 55 - INFO - Resizing screenshot 2\n",
      "2025-04-05 14:59:29,641 - Jupyter Notebook - <module> - Line: 89 - INFO - Completed fold screenshot test. Starting fold test LLM analysis for 2 fold tests\n",
      "2025-04-05 14:59:29,642 - Jupyter Notebook - perform_fold_test_llm - Line: 77 - INFO - Requesting fold test llm process because there are 2 fold tests\n",
      "2025-04-05 14:59:29,643 - helper - analyze_page_actions - Line: 967 - INFO - Sending to Claude LLM API\n",
      "2025-04-05 14:59:30,498 - helper - _run_claude - Line: 867 - INFO - Processing 2 images\n",
      "2025-04-05 14:59:30,500 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\0\\0/fold_test/resized_chunk_1.png ~227.249KB\n",
      "2025-04-05 14:59:30,513 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\0\\0/fold_test/resized_chunk_2.png ~223.25KB\n",
      "2025-04-05 14:59:30,524 - helper - _run_claude - Line: 900 - INFO - Finished processing images\n",
      "2025-04-05 14:59:30,525 - helper - _run_claude - Line: 908 - INFO - Begining Claude LLM API call with 5 message parths and 1141 characters in prompt\n",
      "2025-04-05 14:59:33,368 - helper - _run_claude - Line: 922 - INFO - Claude LLM API call completed\n",
      "2025-04-05 14:59:33,369 - helper - analyze_page_actions - Line: 973 - INFO - Completed Claude LLM API call\n",
      "2025-04-05 14:59:33,371 - helper - perform_llm_fold_test - Line: 1440 - INFO - Completed fold test LLM call\n",
      "2025-04-05 14:59:33,372 - Jupyter Notebook - perform_fold_test_llm - Line: 93 - INFO - Resizing window for best image is resized_chunk_1.png with clarity 5 and 2 relevant elements\n",
      "2025-04-05 14:59:33,462 - Jupyter Notebook - <module> - Line: 107 - INFO - First run in the rerun sequence. Deleting advice\n",
      "2025-04-05 14:59:33,463 - Jupyter Notebook - <module> - Line: 120 - INFO - Copying screenshot from fold test folder to chunk folder for best image for resized and non rezised for best image id 1\n",
      "2025-04-05 14:59:33,468 - Jupyter Notebook - <module> - Line: 166 - INFO - Starting LLM analysis with advice length 0 characters\n",
      "2025-04-05 14:59:33,469 - helper - analyze_page_actions - Line: 967 - INFO - Sending to Claude LLM API\n",
      "2025-04-05 14:59:34,374 - helper - _run_claude - Line: 867 - INFO - Processing 1 images\n",
      "2025-04-05 14:59:34,376 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\0\\0/chunks/resized_chunk_1.png ~227.249KB\n",
      "2025-04-05 14:59:34,388 - helper - _run_claude - Line: 900 - INFO - Finished processing images\n",
      "2025-04-05 14:59:34,390 - helper - _run_claude - Line: 908 - INFO - Begining Claude LLM API call with 3 message parths and 9701 characters in prompt\n",
      "2025-04-05 14:59:50,242 - helper - _run_claude - Line: 922 - INFO - Claude LLM API call completed\n",
      "2025-04-05 14:59:50,245 - helper - analyze_page_actions - Line: 973 - INFO - Completed Claude LLM API call\n",
      "2025-04-05 14:59:50,248 - Jupyter Notebook - <module> - Line: 182 - INFO - Completed LLM analysis. 1 actions to perform\n",
      "2025-04-05 14:59:51,565 - Jupyter Notebook - <module> - Line: 252 - INFO - Sending screenshotting request to API server for 1 actions\n",
      "2025-04-05 14:59:51,943 - Jupyter Notebook - <module> - Line: 265 - INFO - Skipping double checking the image draw dots\n",
      "2025-04-05 14:59:51,944 - Jupyter Notebook - <module> - Line: 268 - INFO - Sending action request to API server for 1 actions\n",
      "2025-04-05 14:59:52,350 - Jupyter Notebook - <module> - Line: 286 - INFO - Executed run actions for 1 actions\n",
      "2025-04-05 14:59:52,351 - Jupyter Notebook - <module> - Line: 294 - INFO - Found 11 out of 1 element metadata in the response\n",
      "2025-04-05 14:59:52,438 - Jupyter Notebook - <module> - Line: 331 - INFO - Creating highlighted screenshot with 1 boxes\n",
      "2025-04-05 14:59:52,559 - helper - draw_with_cairo - Line: 1836 - INFO - Saved visualization to: clients/intuit\\add_cust_v98\\0\\0\\0\\0/highlights/highlights.png\n",
      "2025-04-05 14:59:54,562 - Jupyter Notebook - <module> - Line: 360 - INFO - Requesting extract_metadata with starting width 1256 and starting height 1369\n",
      "2025-04-05 14:59:54,562 - Jupyter Notebook - fold_screenshot_test - Line: 7 - INFO - Requesting extract_metadata for intuit with starting width 1256 and starting height 1369\n",
      "2025-04-05 14:59:56,324 - Jupyter Notebook - fold_screenshot_test - Line: 34 - INFO - Resizing browser to width 1256 and height 1256\n",
      "2025-04-05 14:59:56,407 - Jupyter Notebook - fold_screenshot_test - Line: 38 - INFO - Requesting screenshot for chunk 1\n",
      "2025-04-05 14:59:56,646 - Jupyter Notebook - fold_screenshot_test - Line: 47 - INFO - Completed screenshot for chunk 1\n",
      "2025-04-05 14:59:56,647 - Jupyter Notebook - fold_screenshot_test - Line: 34 - INFO - Resizing browser to width 1256 and height 1256\n",
      "2025-04-05 14:59:56,697 - Jupyter Notebook - fold_screenshot_test - Line: 38 - INFO - Requesting screenshot for chunk 2\n",
      "2025-04-05 14:59:56,964 - Jupyter Notebook - fold_screenshot_test - Line: 47 - INFO - Completed screenshot for chunk 2\n",
      "2025-04-05 14:59:57,050 - Jupyter Notebook - fold_screenshot_test - Line: 55 - INFO - Resizing screenshot 1\n",
      "2025-04-05 14:59:57,185 - Jupyter Notebook - fold_screenshot_test - Line: 55 - INFO - Resizing screenshot 2\n",
      "2025-04-05 14:59:57,320 - Jupyter Notebook - <module> - Line: 364 - INFO - Completed retro/temp fold screenshot test. Starting fold test LLM analysis for 2 fold tests\n",
      "2025-04-05 14:59:57,321 - Jupyter Notebook - perform_fold_test_llm - Line: 77 - INFO - Requesting fold test llm process because there are 2 fold tests\n",
      "2025-04-05 14:59:57,323 - helper - analyze_page_actions - Line: 967 - INFO - Sending to Claude LLM API\n",
      "2025-04-05 14:59:58,252 - helper - _run_claude - Line: 867 - INFO - Processing 2 images\n",
      "2025-04-05 14:59:58,255 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\0\\0/fold_test_temp/resized_chunk_1.png ~287.341KB\n",
      "2025-04-05 14:59:58,276 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\0\\0/fold_test_temp/resized_chunk_2.png ~287.341KB\n",
      "2025-04-05 14:59:58,291 - helper - _run_claude - Line: 900 - INFO - Finished processing images\n",
      "2025-04-05 14:59:58,293 - helper - _run_claude - Line: 908 - INFO - Begining Claude LLM API call with 5 message parths and 1141 characters in prompt\n",
      "2025-04-05 15:00:01,359 - helper - _run_claude - Line: 922 - INFO - Claude LLM API call completed\n",
      "2025-04-05 15:00:01,361 - helper - analyze_page_actions - Line: 973 - INFO - Completed Claude LLM API call\n",
      "2025-04-05 15:00:01,362 - helper - perform_llm_fold_test - Line: 1440 - INFO - Completed fold test LLM call\n",
      "2025-04-05 15:00:01,364 - Jupyter Notebook - perform_fold_test_llm - Line: 93 - INFO - Resizing window for best image is resized_chunk_1.png with clarity 5 and 3 relevant elements\n",
      "2025-04-05 15:00:01,476 - Jupyter Notebook - <module> - Line: 368 - INFO - Completed retro/temp fold test screenshot LLM analysis and the dimensions of the best image ID 1 are {'height': 1256, 'width': 1256}\n",
      "2025-04-05 15:00:01,478 - Jupyter Notebook - <module> - Line: 371 - INFO - Copying screenshot from 'fold_test_temp' folder to 'temp' folder for best image for resized and non rezised for best image id 1\n",
      "2025-04-05 15:00:01,483 - Jupyter Notebook - <module> - Line: 379 - INFO - Starting evaluation of the run screenshots\n",
      "2025-04-05 15:00:01,485 - helper - analyze_page_actions - Line: 967 - INFO - Sending to Claude LLM API\n",
      "2025-04-05 15:00:02,477 - helper - _run_claude - Line: 867 - INFO - Processing 3 images\n",
      "2025-04-05 15:00:02,479 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\0\\0/chunks/resized_chunk_1.png ~227.249KB\n",
      "2025-04-05 15:00:02,481 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\0\\0/dots/resized_final_screenshot.png ~229.899KB\n",
      "2025-04-05 15:00:02,493 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\0\\0/temp/resized_chunk_1.png ~287.341KB\n",
      "2025-04-05 15:00:02,505 - helper - _run_claude - Line: 900 - INFO - Finished processing images\n",
      "2025-04-05 15:00:02,507 - helper - _run_claude - Line: 908 - INFO - Begining Claude LLM API call with 7 message parths and 9030 characters in prompt\n",
      "2025-04-05 15:00:26,132 - helper - _run_claude - Line: 922 - INFO - Claude LLM API call completed\n",
      "2025-04-05 15:00:26,133 - helper - analyze_page_actions - Line: 973 - INFO - Completed Claude LLM API call\n",
      "2025-04-05 15:00:26,134 - Jupyter Notebook - <module> - Line: 391 - INFO - Completed evaluation of the run screenshots\n",
      "2025-04-05 15:00:27,163 - Jupyter Notebook - <module> - Line: 421 - INFO - Goal not achieved. Continuing to next run. Advice lenght is 570 characters and overall summary is 209 characters\n",
      "2025-04-05 15:00:27,480 - Jupyter Notebook - <module> - Line: 472 - INFO - PI: 0010 Progress made. Goal not yet fully achieved. Incrementing run_id to 1 and resetting run_rerun_ct to 0\n",
      "2025-04-05 15:00:27,481 - Jupyter Notebook - <module> - Line: 69 - INFO - Previous run_rerun_path is clients/intuit\\add_cust_v98\\0\\0\\0\\0 and current run_rerun_path is clients/intuit\\add_cust_v98\\0\\0\\1\\0\n",
      "2025-04-05 15:00:27,481 - Jupyter Notebook - <module> - Line: 71 - INFO - Starting run 1 rerun 0 run_rerun 0 in clients/intuit\\add_cust_v98\\0\\0\\1\\0\n",
      "2025-04-05 15:00:27,482 - Jupyter Notebook - <module> - Line: 126 - INFO - Subsequent run in the same rerun sequence. Reusing data from previous run. Advice length is 682 characters\n",
      "2025-04-05 15:00:27,483 - Jupyter Notebook - <module> - Line: 134 - INFO - Copying screenshot from temp to new run folder: clients/intuit\\add_cust_v98\\0\\0\\0\\0/temp to clients/intuit\\add_cust_v98\\0\\0\\1\\0/chunks\n",
      "2025-04-05 15:00:27,486 - Jupyter Notebook - <module> - Line: 166 - INFO - Starting LLM analysis with advice length 682 characters\n",
      "2025-04-05 15:00:27,488 - helper - analyze_page_actions - Line: 967 - INFO - Sending to Claude LLM API\n",
      "2025-04-05 15:00:28,357 - helper - _run_claude - Line: 867 - INFO - Processing 1 images\n",
      "2025-04-05 15:00:28,360 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\1\\0/chunks/resized_chunk_1.png ~287.341KB\n",
      "2025-04-05 15:00:28,371 - helper - _run_claude - Line: 900 - INFO - Finished processing images\n",
      "2025-04-05 15:00:28,373 - helper - _run_claude - Line: 908 - INFO - Begining Claude LLM API call with 3 message parths and 10763 characters in prompt\n",
      "2025-04-05 15:00:47,132 - helper - _run_claude - Line: 922 - INFO - Claude LLM API call completed\n",
      "2025-04-05 15:00:47,134 - helper - analyze_page_actions - Line: 973 - INFO - Completed Claude LLM API call\n",
      "2025-04-05 15:00:47,136 - Jupyter Notebook - <module> - Line: 182 - INFO - Completed LLM analysis. 1 actions to perform\n",
      "2025-04-05 15:00:48,695 - Jupyter Notebook - <module> - Line: 252 - INFO - Sending screenshotting request to API server for 1 actions\n",
      "2025-04-05 15:00:49,044 - Jupyter Notebook - <module> - Line: 265 - INFO - Skipping double checking the image draw dots\n",
      "2025-04-05 15:00:49,045 - Jupyter Notebook - <module> - Line: 268 - INFO - Sending action request to API server for 1 actions\n",
      "2025-04-05 15:00:49,661 - Jupyter Notebook - <module> - Line: 286 - INFO - Executed run actions for 1 actions\n",
      "2025-04-05 15:00:49,662 - Jupyter Notebook - <module> - Line: 294 - INFO - Found 11 out of 1 element metadata in the response\n",
      "2025-04-05 15:00:49,951 - Jupyter Notebook - <module> - Line: 331 - INFO - Creating highlighted screenshot with 1 boxes\n",
      "2025-04-05 15:00:50,312 - helper - draw_with_cairo - Line: 1836 - INFO - Saved visualization to: clients/intuit\\add_cust_v98\\0\\0\\1\\0/highlights/highlights.png\n",
      "2025-04-05 15:00:52,321 - Jupyter Notebook - <module> - Line: 360 - INFO - Requesting extract_metadata with starting width 1256 and starting height 1369\n",
      "2025-04-05 15:00:52,322 - Jupyter Notebook - fold_screenshot_test - Line: 7 - INFO - Requesting extract_metadata for intuit with starting width 1256 and starting height 1369\n",
      "2025-04-05 15:00:53,383 - Jupyter Notebook - fold_screenshot_test - Line: 34 - INFO - Resizing browser to width 1256 and height 1256\n",
      "2025-04-05 15:00:53,435 - Jupyter Notebook - fold_screenshot_test - Line: 38 - INFO - Requesting screenshot for chunk 1\n",
      "2025-04-05 15:00:53,665 - Jupyter Notebook - fold_screenshot_test - Line: 47 - INFO - Completed screenshot for chunk 1\n",
      "2025-04-05 15:00:53,666 - Jupyter Notebook - fold_screenshot_test - Line: 34 - INFO - Resizing browser to width 1256 and height 1256\n",
      "2025-04-05 15:00:53,715 - Jupyter Notebook - fold_screenshot_test - Line: 38 - INFO - Requesting screenshot for chunk 2\n",
      "2025-04-05 15:00:53,933 - Jupyter Notebook - fold_screenshot_test - Line: 47 - INFO - Completed screenshot for chunk 2\n",
      "2025-04-05 15:00:53,996 - Jupyter Notebook - fold_screenshot_test - Line: 55 - INFO - Resizing screenshot 1\n",
      "2025-04-05 15:00:54,135 - Jupyter Notebook - fold_screenshot_test - Line: 55 - INFO - Resizing screenshot 2\n",
      "2025-04-05 15:00:54,274 - Jupyter Notebook - <module> - Line: 364 - INFO - Completed retro/temp fold screenshot test. Starting fold test LLM analysis for 2 fold tests\n",
      "2025-04-05 15:00:54,275 - Jupyter Notebook - perform_fold_test_llm - Line: 77 - INFO - Requesting fold test llm process because there are 2 fold tests\n",
      "2025-04-05 15:00:54,276 - helper - analyze_page_actions - Line: 967 - INFO - Sending to Claude LLM API\n",
      "2025-04-05 15:00:55,197 - helper - _run_claude - Line: 867 - INFO - Processing 2 images\n",
      "2025-04-05 15:00:55,199 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\1\\0/fold_test_temp/resized_chunk_1.png ~201.29KB\n",
      "2025-04-05 15:00:55,210 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\1\\0/fold_test_temp/resized_chunk_2.png ~201.29KB\n",
      "2025-04-05 15:00:55,220 - helper - _run_claude - Line: 900 - INFO - Finished processing images\n",
      "2025-04-05 15:00:55,220 - helper - _run_claude - Line: 908 - INFO - Begining Claude LLM API call with 5 message parths and 1141 characters in prompt\n",
      "2025-04-05 15:01:00,756 - helper - _run_claude - Line: 922 - INFO - Claude LLM API call completed\n",
      "2025-04-05 15:01:00,758 - helper - analyze_page_actions - Line: 973 - INFO - Completed Claude LLM API call\n",
      "2025-04-05 15:01:00,761 - helper - perform_llm_fold_test - Line: 1440 - INFO - Completed fold test LLM call\n",
      "2025-04-05 15:01:00,763 - Jupyter Notebook - perform_fold_test_llm - Line: 93 - INFO - Resizing window for best image is resized_chunk_1.png with clarity 5 and 5 relevant elements\n",
      "2025-04-05 15:01:00,876 - Jupyter Notebook - <module> - Line: 368 - INFO - Completed retro/temp fold test screenshot LLM analysis and the dimensions of the best image ID 1 are {'height': 1256, 'width': 1256}\n",
      "2025-04-05 15:01:00,877 - Jupyter Notebook - <module> - Line: 371 - INFO - Copying screenshot from 'fold_test_temp' folder to 'temp' folder for best image for resized and non rezised for best image id 1\n",
      "2025-04-05 15:01:00,882 - Jupyter Notebook - <module> - Line: 379 - INFO - Starting evaluation of the run screenshots\n",
      "2025-04-05 15:01:00,884 - helper - analyze_page_actions - Line: 967 - INFO - Sending to Claude LLM API\n",
      "2025-04-05 15:01:01,914 - helper - _run_claude - Line: 867 - INFO - Processing 3 images\n",
      "2025-04-05 15:01:01,917 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\1\\0/chunks/resized_chunk_1.png ~287.341KB\n",
      "2025-04-05 15:01:01,920 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\1\\0/dots/resized_final_screenshot.png ~289.43KB\n",
      "2025-04-05 15:01:01,932 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\1\\0/temp/resized_chunk_1.png ~201.29KB\n",
      "2025-04-05 15:01:01,943 - helper - _run_claude - Line: 900 - INFO - Finished processing images\n",
      "2025-04-05 15:01:01,944 - helper - _run_claude - Line: 908 - INFO - Begining Claude LLM API call with 7 message parths and 9383 characters in prompt\n",
      "2025-04-05 15:01:23,320 - helper - _run_claude - Line: 922 - INFO - Claude LLM API call completed\n",
      "2025-04-05 15:01:23,322 - helper - analyze_page_actions - Line: 973 - INFO - Completed Claude LLM API call\n",
      "2025-04-05 15:01:23,326 - Jupyter Notebook - <module> - Line: 391 - INFO - Completed evaluation of the run screenshots\n",
      "2025-04-05 15:01:24,895 - Jupyter Notebook - <module> - Line: 421 - INFO - Goal not achieved. Continuing to next run. Advice lenght is 675 characters and overall summary is 289 characters\n",
      "2025-04-05 15:01:25,464 - Jupyter Notebook - <module> - Line: 472 - INFO - PI: 0020 Progress made. Goal not yet fully achieved. Incrementing run_id to 2 and resetting run_rerun_ct to 0\n",
      "2025-04-05 15:01:25,465 - Jupyter Notebook - <module> - Line: 69 - INFO - Previous run_rerun_path is clients/intuit\\add_cust_v98\\0\\0\\1\\0 and current run_rerun_path is clients/intuit\\add_cust_v98\\0\\0\\2\\0\n",
      "2025-04-05 15:01:25,466 - Jupyter Notebook - <module> - Line: 71 - INFO - Starting run 2 rerun 0 run_rerun 0 in clients/intuit\\add_cust_v98\\0\\0\\2\\0\n",
      "2025-04-05 15:01:25,467 - Jupyter Notebook - <module> - Line: 126 - INFO - Subsequent run in the same rerun sequence. Reusing data from previous run. Advice length is 787 characters\n",
      "2025-04-05 15:01:25,467 - Jupyter Notebook - <module> - Line: 134 - INFO - Copying screenshot from temp to new run folder: clients/intuit\\add_cust_v98\\0\\0\\1\\0/temp to clients/intuit\\add_cust_v98\\0\\0\\2\\0/chunks\n",
      "2025-04-05 15:01:25,471 - Jupyter Notebook - <module> - Line: 166 - INFO - Starting LLM analysis with advice length 787 characters\n",
      "2025-04-05 15:01:25,472 - helper - analyze_page_actions - Line: 967 - INFO - Sending to Claude LLM API\n",
      "2025-04-05 15:01:26,496 - helper - _run_claude - Line: 867 - INFO - Processing 1 images\n",
      "2025-04-05 15:01:26,499 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\2\\0/chunks/resized_chunk_1.png ~201.29KB\n",
      "2025-04-05 15:01:26,514 - helper - _run_claude - Line: 900 - INFO - Finished processing images\n",
      "2025-04-05 15:01:26,515 - helper - _run_claude - Line: 908 - INFO - Begining Claude LLM API call with 3 message parths and 10948 characters in prompt\n",
      "2025-04-05 15:01:54,574 - helper - _run_claude - Line: 922 - INFO - Claude LLM API call completed\n",
      "2025-04-05 15:01:54,576 - helper - analyze_page_actions - Line: 973 - INFO - Completed Claude LLM API call\n",
      "2025-04-05 15:01:54,581 - Jupyter Notebook - <module> - Line: 182 - INFO - Completed LLM analysis. 5 actions to perform\n",
      "2025-04-05 15:02:01,181 - Jupyter Notebook - <module> - Line: 252 - INFO - Sending screenshotting request to API server for 5 actions\n",
      "2025-04-05 15:02:02,222 - Jupyter Notebook - <module> - Line: 265 - INFO - Skipping double checking the image draw dots\n",
      "2025-04-05 15:02:02,223 - Jupyter Notebook - <module> - Line: 268 - INFO - Sending action request to API server for 5 actions\n",
      "2025-04-05 15:02:08,238 - Jupyter Notebook - <module> - Line: 286 - INFO - Executed run actions for 5 actions\n",
      "2025-04-05 15:02:08,239 - Jupyter Notebook - <module> - Line: 294 - INFO - Found 11 out of 5 element metadata in the response\n",
      "2025-04-05 15:02:08,955 - Jupyter Notebook - <module> - Line: 331 - INFO - Creating highlighted screenshot with 5 boxes\n",
      "2025-04-05 15:02:09,110 - helper - draw_with_cairo - Line: 1836 - INFO - Saved visualization to: clients/intuit\\add_cust_v98\\0\\0\\2\\0/highlights/highlights.png\n",
      "2025-04-05 15:02:11,114 - Jupyter Notebook - <module> - Line: 360 - INFO - Requesting extract_metadata with starting width 1256 and starting height 1369\n",
      "2025-04-05 15:02:11,115 - Jupyter Notebook - fold_screenshot_test - Line: 7 - INFO - Requesting extract_metadata for intuit with starting width 1256 and starting height 1369\n",
      "2025-04-05 15:02:12,205 - Jupyter Notebook - fold_screenshot_test - Line: 34 - INFO - Resizing browser to width 1256 and height 1256\n",
      "2025-04-05 15:02:12,282 - Jupyter Notebook - fold_screenshot_test - Line: 38 - INFO - Requesting screenshot for chunk 1\n",
      "2025-04-05 15:02:12,533 - Jupyter Notebook - fold_screenshot_test - Line: 47 - INFO - Completed screenshot for chunk 1\n",
      "2025-04-05 15:02:12,534 - Jupyter Notebook - fold_screenshot_test - Line: 34 - INFO - Resizing browser to width 1256 and height 1256\n",
      "2025-04-05 15:02:12,593 - Jupyter Notebook - fold_screenshot_test - Line: 38 - INFO - Requesting screenshot for chunk 2\n",
      "2025-04-05 15:02:12,899 - Jupyter Notebook - fold_screenshot_test - Line: 47 - INFO - Completed screenshot for chunk 2\n",
      "2025-04-05 15:02:12,984 - Jupyter Notebook - fold_screenshot_test - Line: 55 - INFO - Resizing screenshot 1\n",
      "2025-04-05 15:02:13,151 - Jupyter Notebook - fold_screenshot_test - Line: 55 - INFO - Resizing screenshot 2\n",
      "2025-04-05 15:02:13,294 - Jupyter Notebook - <module> - Line: 364 - INFO - Completed retro/temp fold screenshot test. Starting fold test LLM analysis for 2 fold tests\n",
      "2025-04-05 15:02:13,295 - Jupyter Notebook - perform_fold_test_llm - Line: 77 - INFO - Requesting fold test llm process because there are 2 fold tests\n",
      "2025-04-05 15:02:13,296 - helper - analyze_page_actions - Line: 967 - INFO - Sending to Claude LLM API\n",
      "2025-04-05 15:02:14,273 - helper - _run_claude - Line: 867 - INFO - Processing 2 images\n",
      "2025-04-05 15:02:14,276 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\2\\0/fold_test_temp/resized_chunk_1.png ~193.866KB\n",
      "2025-04-05 15:02:14,287 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\2\\0/fold_test_temp/resized_chunk_2.png ~193.866KB\n",
      "2025-04-05 15:02:14,304 - helper - _run_claude - Line: 900 - INFO - Finished processing images\n",
      "2025-04-05 15:02:14,305 - helper - _run_claude - Line: 908 - INFO - Begining Claude LLM API call with 5 message parths and 1141 characters in prompt\n",
      "2025-04-05 15:02:18,975 - helper - _run_claude - Line: 922 - INFO - Claude LLM API call completed\n",
      "2025-04-05 15:02:18,977 - helper - analyze_page_actions - Line: 973 - INFO - Completed Claude LLM API call\n",
      "2025-04-05 15:02:18,980 - helper - perform_llm_fold_test - Line: 1440 - INFO - Completed fold test LLM call\n",
      "2025-04-05 15:02:18,982 - Jupyter Notebook - perform_fold_test_llm - Line: 93 - INFO - Resizing window for best image is resized_chunk_1.png with clarity 5 and 3 relevant elements\n",
      "2025-04-05 15:02:19,088 - Jupyter Notebook - <module> - Line: 368 - INFO - Completed retro/temp fold test screenshot LLM analysis and the dimensions of the best image ID 1 are {'height': 1256, 'width': 1256}\n",
      "2025-04-05 15:02:19,089 - Jupyter Notebook - <module> - Line: 371 - INFO - Copying screenshot from 'fold_test_temp' folder to 'temp' folder for best image for resized and non rezised for best image id 1\n",
      "2025-04-05 15:02:19,095 - Jupyter Notebook - <module> - Line: 379 - INFO - Starting evaluation of the run screenshots\n",
      "2025-04-05 15:02:19,096 - helper - analyze_page_actions - Line: 967 - INFO - Sending to Claude LLM API\n",
      "2025-04-05 15:02:20,019 - helper - _run_claude - Line: 867 - INFO - Processing 3 images\n",
      "2025-04-05 15:02:20,022 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\2\\0/chunks/resized_chunk_1.png ~201.29KB\n",
      "2025-04-05 15:02:20,027 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\2\\0/dots/resized_final_screenshot.png ~210.786KB\n",
      "2025-04-05 15:02:20,041 - helper - _run_claude - Line: 870 - INFO - Processing image clients/intuit\\add_cust_v98\\0\\0\\2\\0/temp/resized_chunk_1.png ~193.866KB\n",
      "2025-04-05 15:02:20,061 - helper - _run_claude - Line: 900 - INFO - Finished processing images\n",
      "2025-04-05 15:02:20,063 - helper - _run_claude - Line: 908 - INFO - Begining Claude LLM API call with 7 message parths and 11455 characters in prompt\n",
      "2025-04-05 15:02:52,861 - helper - _run_claude - Line: 922 - INFO - Claude LLM API call completed\n",
      "2025-04-05 15:02:52,863 - helper - analyze_page_actions - Line: 973 - INFO - Completed Claude LLM API call\n",
      "2025-04-05 15:02:52,867 - Jupyter Notebook - <module> - Line: 391 - INFO - Completed evaluation of the run screenshots\n",
      "2025-04-05 15:02:56,404 - Jupyter Notebook - <module> - Line: 475 - INFO - PI: 0020 Goal achieved. Ending the workflow.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "import requests\n",
    "\n",
    "max_run = 11\n",
    "max_rerun = 4\n",
    "max_run_rerun = 4\n",
    "\n",
    "max_chunks = 10\n",
    "overlap_percentage = 0\n",
    "draw_dots = True\n",
    "\n",
    "\n",
    "run_id = 0\n",
    "rerun_ct = 0\n",
    "run_rerun_ct = 0\n",
    "advice = ''\n",
    "summary = ''\n",
    "goal_achieved = False\n",
    "any_actions_succeeded = False\n",
    "resize_data = None  # Store resize data between runs\n",
    "\n",
    "# Main loop\n",
    "\n",
    "actions_succeeded = False\n",
    "page_changed = False\n",
    "overall_goal_success = False\n",
    "\n",
    "stability_timeout_ms = 10000\n",
    "stability_window_ms = 1000\n",
    "check_stability_x_before_error = 5\n",
    "\n",
    "wait_after_scroll = 500\n",
    "\n",
    "\n",
    "# 1) Set context\n",
    "sample_id = logger_struct.set_sample()\n",
    "logger_struct.set_rerun(rerun_ct)\n",
    "logger_struct.set_run(run_id)\n",
    "logger_struct.set_run_retry(run_rerun_ct)\n",
    "\n",
    "# todo: \n",
    "# test what happens when a rerun is needed\n",
    "# add \"generalized instructions\" to the actions\n",
    "import math\n",
    "import requests\n",
    "\n",
    "starting_width = 1256\n",
    "starting_height=1369\n",
    "depth_chunks_to_test = 2\n",
    "run_image_double_check = False\n",
    "\n",
    "# dusan_tracker['goal']=workflow_instructions\n",
    "\n",
    "#resize_browser_result = hlp.resize_browser_window(width=browser_start_width, height=browser_start_height)\n",
    "run_rerun_path = ''\n",
    "\n",
    "last_rerun_advice = ''\n",
    "\n",
    "while run_id <= max_run and rerun_ct <= max_rerun and not overall_goal_success and run_rerun_ct <= max_run_rerun:\n",
    "\n",
    "    \n",
    "    #print(logger.context)\n",
    "    # deep copy run_rerun_path\n",
    "    previous_run_rerun_path = copy.deepcopy(run_rerun_path)\n",
    "    #print(f\"Now previous = {previous_run_rerun_path} which is run {run_rerun_path}\")\n",
    "    run_rerun_path = os.path.join(client_folder, workflow_id,str(sample_id),str(rerun_ct),str(run_id),str(run_rerun_ct))\n",
    "    logger.info(f\"Previous run_rerun_path is {previous_run_rerun_path} and current run_rerun_path is {run_rerun_path}\")\n",
    "    #print(f\"And Run is now: {run_rerun_path}\")\n",
    "    logger.info(f\"Starting run {run_id} rerun {rerun_ct} run_rerun {run_rerun_ct} in {run_rerun_path}\")\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Set up folders\n",
    "    \n",
    "    if run_id == 0:\n",
    "        summary = ''\n",
    "\n",
    "\n",
    "\n",
    "        ############################################\n",
    "\n",
    "\n",
    "        logger.info(f\"It's the first run for the sample. Starting fold screenshot test\")\n",
    "        fold_tests,fold_test_paths,screenshot_response,resize_browser_result,url_metadata = fold_screenshot_test(starting_width=starting_width,starting_height=starting_height,depth_chunks_to_test=depth_chunks_to_test,run_rerun_path=run_rerun_path,workflow_instructions=workflow_instructions,site_wide_instructions=site_wide_instructions,site_description=site_description,directory='fold_test')\n",
    "\n",
    "        logger.info(f\"Completed fold screenshot test. Starting fold test LLM analysis for {len(fold_tests)} fold tests\")\n",
    "\n",
    "        page_action_json_ranked_fold_test,best_image,screenshot_data,resize_browser_result = perform_fold_test_llm(fold_tests,fold_test_paths,screenshot_response,resize_browser_result)\n",
    "\n",
    "        ############################################\n",
    "\n",
    "\n",
    "\n",
    "        # First run in a rerun sequence OR stalled - get fresh screenshot and metadata\n",
    "        # NOTICE: this needs to be done only once per rerun sequence (unless the browser is messed with durnin the run)\n",
    "        \n",
    "        # url_metadata, screenshot_data = hlp.run_metadata_gather(\n",
    "        #     output_path=run_rerun_path+\"/chunks\",\n",
    "        #     overlap_percentage=overlap_percentage,\n",
    "        #     max_chunks=max_chunks\n",
    "        # )\n",
    "\n",
    "        if rerun_ct == 0:\n",
    "            logger.info(f\"First run in the rerun sequence. Deleting advice\")\n",
    "            advice = ''\n",
    "        else:\n",
    "            last_rerun_advice = advice # if this is not the first rerun, and is the first run in the rerun, add previous rerun's advice (from the last step/run of the previous rerun) as the last rerun advice to follow\n",
    "\n",
    "        resize_data = {\n",
    "            'resize_browser_result': resize_browser_result,\n",
    "            #'resize_screenshot_result': resize_screenshot_result,\n",
    "            'best_image':best_image,\n",
    "            'url_metadata': url_metadata,\n",
    "            'screenshot_data': screenshot_data\n",
    "        }\n",
    "\n",
    "        logger.info(f\"Copying screenshot from fold test folder to chunk folder for best image for resized and non rezised for best image id {best_image['id']}\")\n",
    "\n",
    "        hlp.copy_and_rename_file(f\"{best_image['path']}\", f\"{run_rerun_path}/chunks/resized_chunk_{best_image['id']}.png\",delete_original=False)\n",
    "        hlp.copy_and_rename_file(f\"{run_rerun_path}/fold_test/chunk_{best_image['id']}.png\", f\"{run_rerun_path}/chunks/chunk_{best_image['id']}.png\",delete_original=False)\n",
    "\n",
    "    else:\n",
    "        logger.info(f\"Subsequent run in the same rerun sequence. Reusing data from previous run. Advice length is {len(advice)} characters\")\n",
    "        # Subsequent runs - copy screenshot from temp and reuse resize data\n",
    "\n",
    "\n",
    "        try: # we write to temp so that we can compare once we have the new screenshot\n",
    "\n",
    "            # temp was captured for eval in the 0th and all other runs\n",
    "\n",
    "            logger.info(f\"Copying screenshot from temp to new run folder: {previous_run_rerun_path}/temp to {run_rerun_path}/chunks\")\n",
    "\n",
    "            if len(previous_run_rerun_path) == 0:\n",
    "                logger.error(f\"Copying screenshot to new run folder: {previous_run_rerun_path} is empty\")\n",
    "            temp_file_list =  hlp.get_files_with_extension(previous_run_rerun_path+\"/temp\", \".png\")\n",
    "            for tf in temp_file_list:\n",
    "                hlp.copy_and_rename_file(\n",
    "                    f'{previous_run_rerun_path}/temp/{tf}',\n",
    "                    f'{run_rerun_path}/chunks/{tf}',\n",
    "                    delete_original=False\n",
    "                )\n",
    "\n",
    "            # Reuse resize data from first run\n",
    "            resize_browser_result = resize_data['resize_browser_result']\n",
    "            #resize_screenshot_result = resize_data['resize_screenshot_result']\n",
    "            best_image = resize_data['best_image']\n",
    "            url_metadata = resize_data['url_metadata']\n",
    "            screenshot_data = resize_data['screenshot_data']\n",
    "            \n",
    "            # hlp.add_step_to_run_data(run_data, 'reused_data', 'Using copied data from previous run', {\n",
    "            #     \"url_metadata\": url_metadata,\n",
    "            #     \"screenshot_data\": screenshot_data,\n",
    "            #     \"resize_screenshot_result\": resize_screenshot_result\n",
    "            # })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error copying screenshot to new run folder: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    logger.info(f\"Starting LLM analysis with advice length {len(advice)} characters\")\n",
    "\n",
    "    page_action_json_ranked,action_prompt = hlp.perform_llm_analysis(\n",
    "        extra_instructions,\n",
    "        site_wide_instructions,site_description,\n",
    "        run_rerun_path,\n",
    "        workflow_instructions,\n",
    "        #resize_screenshot_result, # TODO: change to be broader\n",
    "        best_image,\n",
    "        run_id,\n",
    "        start_time\n",
    "        ,advice\n",
    "        ,last_rerun_advice\n",
    "        ,summary\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Completed LLM analysis. {len(page_action_json_ranked['action_tasks'])} actions to perform\")\n",
    "\n",
    "\n",
    "\n",
    "    if page_action_json_ranked is None:\n",
    "        logger.error(\"LLM analysis failed to produce valid results\")\n",
    "        continue\n",
    "    else:\n",
    "        if len(page_action_json_ranked['action_tasks']) == 0:\n",
    "            logger.error(\"LLM analysis failed to produce valid results\")\n",
    "            continue\n",
    "    #hlp.add_step_to_run_data(run_data, 'llm_analysis', 'LLM analysis for action planning', page_action_json_ranked)\n",
    "\n",
    "    # print(\"\\nDrawing elements and creating highlighted screenshot...\")\n",
    "    # print(f\"Page action JSON: {json.dumps(page_action_json_ranked, indent=2)}\")\n",
    "\n",
    "\n",
    "    logger_struct.set_run_retry_value(\"summary_of_steps_so_far\", page_action_json_ranked['summary_of_steps_so_far'])\n",
    "    logger_struct.set_run_retry_value(\"expected_outcome_hopeful\", page_action_json_ranked['expected_outcome_hopeful'])\n",
    "    logger_struct.set_run_retry_value(\"_advice_assessment\", page_action_json_ranked['_advice_assessment'])\n",
    "    logger_struct.set_run_retry_value(\"page_description\", page_action_json_ranked['page_description'])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "## left off\n",
    "    \n",
    "    # Prepare actions to perform\n",
    "    actions_to_perform = []\n",
    "    for task in page_action_json_ranked.get('action_tasks', []):\n",
    "        logger_struct.set_action(task['action_id'])\n",
    "        logger_struct.set_action_value(\"description\", task['description'])\n",
    "        # Fix the string concatenation by ensuring proper order of operations\n",
    "        # dusan_tracker.append(\"    -Action tasks: ID:\" + str(task['action_id']) + \" Descript: \" + str(task['description']))\n",
    "\n",
    "        for c in task['candidates']:\n",
    "            logger_struct.set_candidate(c['candidate_id'])\n",
    "            logger_struct.set_candidate_value(key=\"element_description\", value=c['element_description'])\n",
    "            logger_struct.set_candidate_value(key=\"_need_to_click_dropdown_arrow\",value=c['_need_to_click_dropdown_arrow'])\n",
    "            logger_struct.set_candidate_value(key=\"type_text\", value=c.get('type_text', ''))\n",
    "            logger_struct.set_candidate_value(key=\"keyboard_action\", value=c.get('keyboard_action', ''))\n",
    "            logger_struct.set_candidate_value(key=\"image_number\", value=c.get('image_number', ''))\n",
    "            logger_struct.set_candidate_value(key=\"action\", value=c['action'])\n",
    "            logger_struct.set_candidate_value(key=\"to_act\", value=c['to_act'])\n",
    "\n",
    "        action_candidates = {\n",
    "            'description': task['description'],\n",
    "            'action_id': task['action_id'],\n",
    "            'candidates': [c for c in task['candidates'] if c.get('to_act', False)]\n",
    "        }\n",
    "\n",
    "        actions_to_perform.append(action_candidates)\n",
    "\n",
    "\n",
    "    # Prepare the action request payload\n",
    "    action_request_payload = {\n",
    "        'multiple_steps_required': page_action_json_ranked.get('multiple_steps_required', False),\n",
    "        'visible_elements_from_instructions': page_action_json_ranked.get('visible_elements_from_instructions', ''),\n",
    "        'summary_of_steps_so_far': page_action_json_ranked.get('summary_of_steps_so_far', ''),\n",
    "        'action_tasks': actions_to_perform,\n",
    "        'error': page_action_json_ranked.get('error', None),\n",
    "        'page_description': page_action_json_ranked.get('page_description', ''),\n",
    "        'expected_outcome_hopeful': page_action_json_ranked.get('expected_outcome_hopeful', '')\n",
    "    }\n",
    "\n",
    "\n",
    "    # Execute actions\n",
    "    #response = hlp.execute_actions(action_request_payload, 2)  # The 2 is the wait_time parameter\n",
    "\n",
    "    # Execute actions using the API server (FastAPI).\n",
    "    logger.info(f\"Sending screenshotting request to API server for {len(action_request_payload['action_tasks'])} actions\")\n",
    "    draw_response = hlp.send_action_request(action_request_payload, wait_time=2,\n",
    "        run_rerun_path=run_rerun_path,draw_no_action=True)\n",
    "    \n",
    "\n",
    "    if run_image_double_check:\n",
    "        logger.info(f\"Double checking the image draw dots\")\n",
    "        coordinate_info_to_review = []\n",
    "        for a in action_request_payload['action_tasks']:\n",
    "            coordinate_info_to_review.append({'action_id':a['action_id'],'description':a['description'],'element_description':a['candidates'][0]['element_description'],'coordinates':a['candidates'][0]['coordinates_ready_to_act'],'action':a['candidates'][0]['action']})\n",
    "\n",
    "        coor_check_results,coor_check_prompt,coor_check_raw = hlp.check_coordinates_llm(workflow_instructions,site_wide_instructions,site_description,run_rerun_path,coordinate_info_to_review)\n",
    "    else:\n",
    "        logger.info(f\"Skipping double checking the image draw dots\")\n",
    "    \n",
    "\n",
    "    logger.info(f\"Sending action request to API server for {len(action_request_payload['action_tasks'])} actions\")\n",
    "    action_response = hlp.send_action_request(action_request_payload, wait_time=2,\n",
    "        run_rerun_path=run_rerun_path,draw_no_action=False)\n",
    "\n",
    "    # not shure if this is needed\n",
    "    hlp.resize_and_crop(\n",
    "            f'{run_rerun_path}/dots/final_screenshot.png',\n",
    "            f'{run_rerun_path}/dots/resized_final_screenshot.png'\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    actions_succeeded = False\n",
    "    page_changed = False\n",
    "    goal_achieved = False\n",
    "    overall_goal_success = False\n",
    "\n",
    "    if action_response:\n",
    "        logger.info(f\"Executed run actions for {len(action_response)} actions\")\n",
    "\n",
    "\n",
    "        if type(action_response[0]['element_metadata'])!=dict:\n",
    "            logger.error(\"Element metadata is not a dictionary\")\n",
    "        elif len(action_response[0]['element_metadata']) == 0:\n",
    "            logger.error(\"No element metadata in the response\")\n",
    "        else: # Store element metadata for each action and candidate in the logger struct and the log file for the run retry\n",
    "            logger.info(f\"Found {len(action_response[0]['element_metadata'])} out of {len(action_response)} element metadata in the response\")\n",
    "            # Add element metadata to logger_struct in sync with existing matching logic\n",
    "            for at in page_action_json_ranked['action_tasks']:\n",
    "                for c in at['candidates']:\n",
    "                    for r in action_response:\n",
    "                        if r['action_id'] == at['action_id'] and r['candidate_id'] == c['candidate_id']:\n",
    "                            logger_struct.set_action(r['action_id'])\n",
    "                            logger_struct.set_candidate(r['candidate_id'])\n",
    "                            logger_struct.set_candidate_value(key=\"element_metadata\", value=r['element_metadata'])\n",
    "\n",
    "\n",
    "        highlighting_metadata = {}\n",
    "        for at in page_action_json_ranked['action_tasks']:\n",
    "            #print(at)\n",
    "            for c in at['candidates']:\n",
    "                #print(c)\n",
    "                for r in action_response:\n",
    "                    if r['action_id'] == at['action_id'] and r['candidate_id'] == c['candidate_id']:\n",
    "                        # dusan_tracker.append(\"         -Acted on Action tasks: aID:\" + str(r['action_id']) + \" cID:\" + str(r['candidate_id']))\n",
    "\n",
    "                        \n",
    "\n",
    "\n",
    "                        if c.get('action','') != 'keyboard_action' and (c.get(\"keyboard_action\", '') is None or c.get(\"keyboard_action\", '').strip()==''):\n",
    "                            c['element_metadata'] = r['element_metadata']\n",
    "                            new_box = copy.deepcopy(c['element_metadata']['boundingBox'])\n",
    "                            #new_box['y'] = new_box['y'] - c['scroll_to']\n",
    "\n",
    "                            if at['action_id'] in highlighting_metadata:\n",
    "                                highlighting_metadata[at['action_id']].append(new_box)\n",
    "                                \n",
    "                                    \n",
    "                            else:\n",
    "                                highlighting_metadata[at['action_id']] = [new_box]\n",
    "                        else:\n",
    "                            c['element_metadata'] = {}\n",
    "        \n",
    "        logger.info(f\"Creating highlighted screenshot with {len(highlighting_metadata)} boxes\")\n",
    "\n",
    "        file_for_cairo = next((s for s in hlp.get_files_with_extension(run_rerun_path+\"/chunks\", \".png\") if s.startswith(\"chunk\")), None)\n",
    "        hlp.create_highlighted_screenshot_cairo(\n",
    "            run_rerun_path=run_rerun_path, # Use the current\n",
    "            bbox_payload = highlighting_metadata\n",
    "            ,file_for_cairo = file_for_cairo\n",
    "        )\n",
    "        # check for stability before proceeding by checking if the page is stable\n",
    "\n",
    "        # todo: create stability check\n",
    "        # sleep for 5 seconds\n",
    "        time.sleep(2)       \n",
    "        # stability = False\n",
    "        # for i in range(check_stability_x_before_error):\n",
    "\n",
    "        #     screenshot_response = requests.get(\n",
    "        #         f\"http://127.0.0.1:8000/check-stability?timeout_ms={stability_timeout_ms}&window_size_ms={stability_window_ms}\",\n",
    "        #     )\n",
    "        #     if screenshot_response.json()['is_stable']:\n",
    "        #         stability = True\n",
    "        #         break\n",
    "        # if not stability:\n",
    "        #     print(\"Error!!!!!!!!!!!!!!!!!!!: Stability not reached\")\n",
    "        #     break\n",
    "\n",
    "########################################################\n",
    "   \n",
    "\n",
    "        logger.info(f\"Requesting extract_metadata with starting width {starting_width} and starting height {starting_height}\")\n",
    "\n",
    "        fold_tests_temp,fold_test_paths_temp,screenshot_response_temp,resize_browser_result_temp,url_metadata_temp = fold_screenshot_test(starting_width=starting_width,starting_height=starting_height,depth_chunks_to_test=depth_chunks_to_test,run_rerun_path=run_rerun_path,workflow_instructions=workflow_instructions,site_wide_instructions=site_wide_instructions,site_description=site_description,directory='fold_test_temp')\n",
    "\n",
    "        logger.info(f\"Completed retro/temp fold screenshot test. Starting fold test LLM analysis for {len(fold_tests_temp)} fold tests\")\n",
    "\n",
    "        page_action_json_ranked_fold_test_temp,best_image_temp,screenshot_data_temp,resize_browser_result_temp = perform_fold_test_llm(fold_tests_temp,fold_test_paths_temp,screenshot_response_temp,resize_browser_result_temp)\n",
    "        \n",
    "        logger.info(f\"Completed retro/temp fold test screenshot LLM analysis and the dimensions of the best image ID {best_image_temp['id']} are {best_image_temp['original_dimensions']}\")\n",
    "\n",
    "\n",
    "        logger.info(f\"Copying screenshot from 'fold_test_temp' folder to 'temp' folder for best image for resized and non rezised for best image id {best_image['id']}\")\n",
    "\n",
    "        hlp.copy_and_rename_file(f\"{best_image_temp['path']}\", f\"{run_rerun_path}/temp/resized_chunk_{best_image_temp['id']}.png\",delete_original=False)\n",
    "        hlp.copy_and_rename_file(f\"{run_rerun_path}/fold_test_temp/chunk_{best_image_temp['id']}.png\", f\"{run_rerun_path}/temp/chunk_{best_image_temp['id']}.png\",delete_original=False)\n",
    "\n",
    "############# STOPED HERE XISOFYSFE\n",
    "        previous_step_detail = hlp.extract_entities_for_evaluation(page_action_json_ranked)\n",
    "\n",
    "        logger.info(f\"Starting evaluation of the run screenshots\")\n",
    "\n",
    "        eval_json = hlp.evaluate_run_screenshots(\n",
    "            workflow_instructions,\n",
    "            site_wide_instructions,site_description,\n",
    "            run_rerun_path,\n",
    "            #[resize_screenshot_result, resize_screenshot_result_temp],\n",
    "            [best_image,best_image_temp],\n",
    "            previous_step_detail,\n",
    "            summary,\n",
    "            start_time\n",
    "        )\n",
    "        logger.info(f\"Completed evaluation of the run screenshots\")\n",
    "\n",
    "        logger_struct.append_to_open_log_run_retry(\"Evaluation completed\")\n",
    "\n",
    "        actions_succeeded = eval_json['actions_succeeded']\n",
    "        page_changed = eval_json['page_changed']\n",
    "        overall_goal_success = eval_json['overall_goal_success']\n",
    "\n",
    "        logger_struct.set_run_retry_value(\"actions_succeeded\", eval_json['actions_succeeded'])\n",
    "        logger_struct.set_run_retry_value(\"page_changed\", eval_json['page_changed'])\n",
    "        logger_struct.set_run_retry_value(\"overall_goal_success\", eval_json['overall_goal_success'])\n",
    "\n",
    "        logger_struct.set_run_retry_value(\"summary\", eval_json['summary'])\n",
    "        logger_struct.set_run_retry_value(\"run_advice\", eval_json['run_advice'])\n",
    "        logger_struct.set_run_retry_value(\"_expectations_analysis\", eval_json['_expectations_analysis'])\n",
    "        logger_struct.set_run_retry_value(\"_precision_analysis\", eval_json['_precision_analysis'])\n",
    "        logger_struct.set_run_retry_value(\"_error_type\", eval_json['_error_type'])\n",
    "\n",
    "        for a in eval_json['action_tasks']:\n",
    "            logger_struct.set_action(a['action_id'])\n",
    "            logger_struct.set_action_value(key=\"_error_type\", value=c.get('_error_type', ''))\n",
    "\n",
    "            for c in a.get('candidates', []):\n",
    "                logger_struct.set_candidate(c['candidate_id'])\n",
    "                logger_struct.set_candidate_value(key=\"success\", value=c.get('success', ''))\n",
    "                logger_struct.set_candidate_value(key=\"issues\", value=c.get('issues', ''))\n",
    "                logger_struct.set_candidate_value(key=\"_candidate_advice\", value=c.get('_candidate_advice', ''))\n",
    "\n",
    "\n",
    "        if not overall_goal_success:\n",
    "            logger.info(f\"Goal not achieved. Continuing to next run. Advice lenght is {len(eval_json['run_advice'])} characters and overall summary is {len(eval_json['summary'])} characters\")\n",
    "            # dusan_tracker.append(\"    -didn't succeed in goal\")\n",
    "            advice = f\"ADVICE FROM PREVIOUS STEPS OR RUNS: Consider this advice from previous attempts to guide you what to do next.: {eval_json['run_advice']}\\n\"\n",
    "            summary = f\"OVERALL PROCESS SUMMARY: Consider this summary from previous attempts to guide you what to do next.: {eval_json['summary']}\\n\"\n",
    "\n",
    "    else:\n",
    "        logger.error(\"No response from the action API\")\n",
    "\n",
    "    # track_data[\"runs\"].append(run_data)\n",
    "    # hlp.write_track_data(track_data, folder_structures['workflow_path'])\n",
    "    \n",
    "\n",
    "\n",
    "    # Update run_id based on success of actions\n",
    "    # If any actions succeeded and can't contune, increment run_id\n",
    "    if not actions_succeeded and page_changed:  # If we cannot continue (major error)\n",
    "\n",
    "        logger_struct.append_to_open_log_run_retry(\"Run needs to reset. Incremeenting rerun count.\")\n",
    "\n",
    "        rerun_ct += 1\n",
    "        run_id = 0\n",
    "        run_rerun_ct = 0\n",
    "        logger_struct.set_rerun(rerun_ct)\n",
    "        logger_struct.set_run(run_id)\n",
    "        logger_struct.set_run_retry(run_rerun_ct)\n",
    "\n",
    "        logger.info(f\"PI: {sample_id}{rerun_ct}{run_id}{run_rerun_ct} Run needs to reset because actions didn't succeed and page was changed. Incrementing rerun count to {rerun_ct}. Resetting run_id to {run_id} and run_rerun_ct to {run_rerun_ct}. Navigating to initial state url metadata\")\n",
    "\n",
    "        any_actions_succeeded = False\n",
    "\n",
    "        # dusan_tracker.append(\"    -Cannot continue, resetting memory, rerun count: \" + str(rerun_ct) + \"navigation to initial state url metadata\")\n",
    "\n",
    "\n",
    "        hlp.navigate_to_url_from_metadata(client_url)\n",
    "\n",
    "        # Do NOT increment run_id here!\n",
    "    elif actions_succeeded and page_changed:  # If we can continue (minor or no changes)\n",
    "        \n",
    "\n",
    "        logger_struct.append_to_open_log_run_retry(\"Action succeeded, page changed.\")\n",
    "        # dusan_tracker.append(f\"    -Can continue actions_succeeded: {actions_succeeded}, page_changed: {page_changed}\")\n",
    "        if not overall_goal_success:  \n",
    "            \n",
    "            \n",
    "            logger_struct.append_to_open_log_run_retry(\"Goal not yet achieved\")\n",
    "            # dusan_tracker.append(f\"    -Goal not achieved but progress made, goal_achieved: {goal_achieved}, actions_succeeded: {actions_succeeded}, page_changed: {page_changed}\")\n",
    "            run_id += 1  # Move to the next run (correct)\n",
    "            logger_struct.set_run(run_id)\n",
    "            run_rerun_ct = 0\n",
    "            logger_struct.set_run_retry(run_rerun_ct)\n",
    "            \n",
    "            logger.info(f\"PI: {sample_id}{rerun_ct}{run_id}{run_rerun_ct} Progress made. Goal not yet fully achieved. Incrementing run_id to {run_id} and resetting run_rerun_ct to {run_rerun_ct}\")\n",
    "        else:\n",
    "            \n",
    "            logger.info(f\"PI: {sample_id}{rerun_ct}{run_id}{run_rerun_ct} Goal achieved. Ending the workflow.\")\n",
    "            logger_struct.append_to_open_log_run_retry(\"Goal achieved\")\n",
    "    else: # If we can continue but the page hasn't changed\n",
    "        \n",
    "        logger_struct.append_to_open_log_run_retry(\"Page hasn't changed, but can continue at current step.\")\n",
    "        #dont change the run_id but increment the run rerun count\n",
    "        # dusan_tracker.append(f\"    -Page hasn't changed, goal_achieved: {goal_achieved}, actions_succeeded: {actions_succeeded}, page_changed: {page_changed}\")\n",
    "        run_rerun_ct += 1\n",
    "        \n",
    "        logger.info(f\"PI: {sample_id}{rerun_ct}{run_id}{run_rerun_ct} Page hasn't changed, but can continue at current step. Incrementing run_rerun_ct to {run_rerun_ct}\")\n",
    "\n",
    "        logger_struct.set_run_retry(run_rerun_ct)\n",
    "    \n",
    "\n",
    "    #todo\n",
    "    # why do I still get Error in main loop: Key 'summary_of_steps_so_far' already exists in this node; cannot overwrite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def _convert_datetime_strings_to_objects(data_dict):\n",
    "    \"\"\"Recursively convert ISO format datetime strings to datetime objects.\"\"\"\n",
    "    if isinstance(data_dict, dict):\n",
    "        if \"created_at\" in data_dict and isinstance(data_dict[\"created_at\"], str):\n",
    "            try:\n",
    "                data_dict[\"created_at\"] = datetime.datetime.fromisoformat(data_dict[\"created_at\"])\n",
    "            except ValueError:\n",
    "                # Handle cases where the string might not be a valid ISO format\n",
    "                # Or decide to keep it as a string if conversion fails\n",
    "                pass\n",
    "        for value in data_dict.values():\n",
    "            _convert_datetime_strings_to_objects(value)\n",
    "    elif isinstance(data_dict, list):\n",
    "        for item in data_dict:\n",
    "            _convert_datetime_strings_to_objects(item)\n",
    "\n",
    "def _convert_datetime_objects_to_strings(data_dict):\n",
    "    \"\"\"Create a copy of the data with datetime objects converted to ISO format strings.\"\"\"\n",
    "    if isinstance(data_dict, dict):\n",
    "        result = {}\n",
    "        for key, value in data_dict.items():\n",
    "            if isinstance(value, datetime.datetime):\n",
    "                result[key] = value.isoformat()\n",
    "            else:\n",
    "                result[key] = _convert_datetime_objects_to_strings(value)\n",
    "        return result\n",
    "    elif isinstance(data_dict, list):\n",
    "        return [_convert_datetime_objects_to_strings(item) for item in data_dict]\n",
    "    else:\n",
    "        return data_dict\n",
    "\n",
    "# --- Main Logic ---\n",
    "\n",
    "def analyze_workflow_logs(json_file_path, target_workflow_id):\n",
    "    \"\"\"\n",
    "    Analyzes the logger JSON for a specific workflow, calculates stats,\n",
    "    and extracts cleaned successful paths.\n",
    "\n",
    "    Args:\n",
    "        json_file_path (str or Path): Path to the logger.json file.\n",
    "        target_workflow_id (str): The ID of the workflow to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing analysis results, including stats\n",
    "              and cleaned successful rerun data, or an error message.\n",
    "              Returns None if the file cannot be read or parsed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            # Optional: Convert loaded datetime strings if needed for processing\n",
    "            # _convert_datetime_strings_to_objects(data)\n",
    "    except FileNotFoundError:\n",
    "        return {\"error\": f\"File not found: {json_file_path}\"}\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": f\"Invalid JSON format in file: {json_file_path}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"An error occurred reading the file: {e}\"}\n",
    "\n",
    "    if target_workflow_id not in data.get(\"workflows\", {}):\n",
    "        return {\"error\": f\"Workflow ID '{target_workflow_id}' not found.\"}\n",
    "\n",
    "    workflow_data = data[\"workflows\"][target_workflow_id]\n",
    "    analysis_results = {\n",
    "        \"workflow_id\": target_workflow_id,\n",
    "        \"analysis_timestamp\": datetime.datetime.utcnow().isoformat(),\n",
    "        \"samples\": {}\n",
    "    }\n",
    "\n",
    "    for sample_id, sample_data in workflow_data.get(\"samples\", {}).items():\n",
    "        sample_analysis = {\n",
    "            \"sample_id\": sample_id,\n",
    "            \"rerun_stats\": {},\n",
    "            \"cleaned_successful_reruns\": {} # Store cleaned paths here\n",
    "        }\n",
    "\n",
    "        for rerun_id, rerun_data in sample_data.get(\"reruns\", {}).items():\n",
    "            rerun_stats = {\n",
    "                \"rerun_id\": rerun_id,\n",
    "                \"is_successful\": False, # Overall goal achieved in *any* retry of *any* run\n",
    "                \"total_runs\": 0,\n",
    "                \"successful_runs_count\": 0, # Count of runs where *any* retry had actions_succeeded=True\n",
    "                \"successful_run_ids\": [],\n",
    "                \"final_successful_run_retry_path\": None # Path (rerun, run, retry) of the first retry achieving the goal\n",
    "            }\n",
    "            cleaned_runs_for_this_rerun = {} # Temp storage for cleaned runs of this rerun\n",
    "\n",
    "            rerun_achieved_goal = False\n",
    "            first_goal_achieving_path = None\n",
    "\n",
    "            for run_id, run_data in rerun_data.get(\"runs\", {}).items():\n",
    "                rerun_stats[\"total_runs\"] += 1\n",
    "                run_had_successful_actions = False\n",
    "                highest_successful_retry_id_int = -1\n",
    "                successful_retry_node_for_cleaning = None # The node to clean for this run\n",
    "\n",
    "                for run_retry_id, run_retry_data in run_data.get(\"run_retries\", {}).items():\n",
    "                    # Check if this retry achieved the overall goal for the rerun\n",
    "                    if run_retry_data.get(\"overall_goal_success\", False):\n",
    "                        rerun_achieved_goal = True\n",
    "                        # Store the path of the *first* retry that achieved the goal\n",
    "                        if first_goal_achieving_path is None:\n",
    "                             first_goal_achieving_path = (rerun_id, run_id, run_retry_id)\n",
    "\n",
    "                    # Check if this retry had successful actions (for run success status and cleaning)\n",
    "                    if run_retry_data.get(\"actions_succeeded\", False):\n",
    "                        run_had_successful_actions = True\n",
    "                        try:\n",
    "                            current_retry_id_int = int(run_retry_id)\n",
    "                            # Keep track of the highest successful retry for cleaning purposes\n",
    "                            if current_retry_id_int > highest_successful_retry_id_int:\n",
    "                                highest_successful_retry_id_int = current_retry_id_int\n",
    "                                successful_retry_node_for_cleaning = run_retry_data\n",
    "                        except ValueError:\n",
    "                            print(f\"Warning: Non-integer run_retry_id '{run_retry_id}' found in {target_workflow_id}/{sample_id}/{rerun_id}/{run_id}. Skipping for cleaning.\")\n",
    "                            continue # Skip this retry for cleaning if ID isn't an int\n",
    "\n",
    "                # --- Update Run Stats ---\n",
    "                if run_had_successful_actions:\n",
    "                    rerun_stats[\"successful_runs_count\"] += 1\n",
    "                    rerun_stats[\"successful_run_ids\"].append(run_id)\n",
    "\n",
    "                    # --- Build Cleaned Run (if run had successful actions) ---\n",
    "                    if successful_retry_node_for_cleaning:\n",
    "                        # 1. Deep copy the successful retry node\n",
    "                        cleaned_run_retry_node = copy.deepcopy(successful_retry_node_for_cleaning)\n",
    "\n",
    "                        # 2. Filter actions within the cleaned retry node\n",
    "                        cleaned_actions = {}\n",
    "                        if \"actions\" in cleaned_run_retry_node:\n",
    "                            for action_id, action_data in cleaned_run_retry_node[\"actions\"].items():\n",
    "                                cleaned_candidates = {}\n",
    "                                action_should_be_kept = False\n",
    "                                if \"candidates\" in action_data:\n",
    "                                    for candidate_id, candidate_data in action_data[\"candidates\"].items():\n",
    "                                        # Keep candidate ONLY if it was acted upon AND successful\n",
    "                                        if candidate_data.get(\"to_act\", False) and candidate_data.get(\"success\", False):\n",
    "                                            cleaned_candidates[candidate_id] = copy.deepcopy(candidate_data)\n",
    "                                            action_should_be_kept = True # Mark action for keeping\n",
    "\n",
    "                                # If at least one successful candidate was found, keep the action\n",
    "                                if action_should_be_kept:\n",
    "                                    cleaned_action_node = copy.deepcopy(action_data) # Keep action metadata\n",
    "                                    cleaned_action_node[\"candidates\"] = cleaned_candidates\n",
    "                                    cleaned_actions[action_id] = cleaned_action_node\n",
    "\n",
    "                        cleaned_run_retry_node[\"actions\"] = cleaned_actions # Replace actions with cleaned ones\n",
    "\n",
    "                        # 3. Create the cleaned run structure\n",
    "                        cleaned_run_node = copy.deepcopy(run_data) # Keep run metadata\n",
    "                        # Store ONLY the cleaned successful retry under its original ID (as string)\n",
    "                        cleaned_run_node[\"run_retries\"] = {str(highest_successful_retry_id_int): cleaned_run_retry_node}\n",
    "                        cleaned_runs_for_this_rerun[run_id] = cleaned_run_node\n",
    "\n",
    "\n",
    "            # --- Finalize Rerun Stats ---\n",
    "            rerun_stats[\"is_successful\"] = rerun_achieved_goal\n",
    "            if rerun_achieved_goal:\n",
    "                rerun_stats[\"final_successful_run_retry_path\"] = first_goal_achieving_path\n",
    "\n",
    "            if rerun_stats[\"total_runs\"] > 0:\n",
    "                rerun_stats[\"success_percentage\"] = (rerun_stats[\"successful_runs_count\"] / rerun_stats[\"total_runs\"]) * 100.0\n",
    "            else:\n",
    "                rerun_stats[\"success_percentage\"] = 0.0\n",
    "\n",
    "            sample_analysis[\"rerun_stats\"][rerun_id] = rerun_stats\n",
    "\n",
    "            # --- Store Cleaned Rerun (if successful overall) ---\n",
    "            if rerun_stats[\"is_successful\"]:\n",
    "                 # Create the cleaned rerun structure\n",
    "                 cleaned_rerun_node = copy.deepcopy(rerun_data) # Keep rerun metadata\n",
    "                 cleaned_rerun_node[\"runs\"] = cleaned_runs_for_this_rerun # Add the cleaned runs\n",
    "                 sample_analysis[\"cleaned_successful_reruns\"][rerun_id] = cleaned_rerun_node\n",
    "\n",
    "\n",
    "        analysis_results[\"samples\"][sample_id] = sample_analysis\n",
    "\n",
    "    # Optional: Convert datetime objects back to strings for JSON saving if needed elsewhere\n",
    "    # analysis_results_serializable = _convert_datetime_objects_to_strings(analysis_results)\n",
    "    # return analysis_results_serializable\n",
    "\n",
    "    return analysis_results\n",
    "\n",
    "\n",
    "def transform_to_guided_experience(analysis_results, original_workflow_data):\n",
    "    \"\"\"\n",
    "    Transforms the cleaned successful rerun data from analysis_results\n",
    "    into the specified guided experience JSON format.\n",
    "\n",
    "    Args:\n",
    "        analysis_results (dict): The output dictionary from analyze_workflow_logs.\n",
    "        original_workflow_data (dict): The original data for the specific workflow\n",
    "                                       from logger.json (needed for instructions).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary representing the guided experience JSON, or None if\n",
    "              no successful path is found for any sample.\n",
    "    \"\"\"\n",
    "    guided_experiences = {} # Store results per sample\n",
    "\n",
    "    workflow_id = analysis_results.get(\"workflow_id\")\n",
    "    if not workflow_id:\n",
    "        print(\"Error: Workflow ID missing in analysis results.\")\n",
    "        return None\n",
    "\n",
    "    for sample_id, sample_analysis in analysis_results.get(\"samples\", {}).items():\n",
    "        # Find the first successful rerun for this sample\n",
    "        successful_rerun_id = None\n",
    "        cleaned_rerun_node = None\n",
    "        if sample_analysis.get(\"cleaned_successful_reruns\"):\n",
    "            # Get the first key (rerun_id) from the cleaned reruns dictionary\n",
    "            successful_rerun_id = next(iter(sample_analysis[\"cleaned_successful_reruns\"]))\n",
    "            cleaned_rerun_node = sample_analysis[\"cleaned_successful_reruns\"][successful_rerun_id]\n",
    "\n",
    "        if not cleaned_rerun_node:\n",
    "            print(f\"Warning: No cleaned successful rerun found for sample {sample_id}.\")\n",
    "            continue # Skip this sample if no successful path was cleaned\n",
    "\n",
    "        # --- Start building the guided experience structure ---\n",
    "        experience_id = f\"{workflow_id}-guide-{sample_id}-{successful_rerun_id}\" # Example ID generation\n",
    "        guided_experience = {\n",
    "            \"id\": experience_id,\n",
    "            \"description\": original_workflow_data.get(\"workflow_instructions\", f\"Guided experience for {workflow_id}\"),\n",
    "            \"topics\": [], # As requested\n",
    "            \"few_shot_examples\": [], # As requested\n",
    "            \"intent\": workflow_id, # Using workflow_id as intent\n",
    "            \"steps\": []\n",
    "        }\n",
    "\n",
    "        # --- Iterate through the cleaned runs and actions to build steps ---\n",
    "        # Sort runs by run_id (assuming they are numeric strings)\n",
    "        sorted_run_ids = sorted(cleaned_rerun_node.get(\"runs\", {}).keys(), key=int)\n",
    "\n",
    "        for run_id in sorted_run_ids:\n",
    "            cleaned_run_node = cleaned_rerun_node[\"runs\"][run_id]\n",
    "            # There should only be one retry_id in the cleaned data\n",
    "            if not cleaned_run_node.get(\"run_retries\"):\n",
    "                continue\n",
    "            run_retry_id = next(iter(cleaned_run_node[\"run_retries\"])) # Get the single retry ID\n",
    "            cleaned_run_retry_node = cleaned_run_node[\"run_retries\"][run_retry_id]\n",
    "\n",
    "            # Sort actions by action_id (assuming they are numeric strings)\n",
    "            sorted_action_ids = sorted(cleaned_run_retry_node.get(\"actions\", {}).keys(), key=int)\n",
    "\n",
    "            for action_id in sorted_action_ids:\n",
    "                cleaned_action_node = cleaned_run_retry_node[\"actions\"][action_id]\n",
    "                # There should only be one candidate_id in the cleaned data\n",
    "                if not cleaned_action_node.get(\"candidates\"):\n",
    "                    continue\n",
    "                candidate_id = next(iter(cleaned_action_node[\"candidates\"])) # Get the single candidate ID\n",
    "                cleaned_candidate_node = cleaned_action_node[\"candidates\"][candidate_id]\n",
    "\n",
    "                # --- Extract data for the step ---\n",
    "                step_id = f\"{experience_id}-step-{action_id}\" # Example step ID\n",
    "                step_text = cleaned_action_node.get(\"description\", \"No action description provided.\")\n",
    "                step_friendly_text = cleaned_candidate_node.get(\"element_description\", \"No element description provided.\")\n",
    "                step_audio_id = step_id # Using step_id as audio_id\n",
    "\n",
    "                # --- Extract mapping info (handle missing data) ---\n",
    "                ui_selector = None\n",
    "                identifier = None\n",
    "                action_type = cleaned_candidate_node.get(\"action\", \"unknown\") # Default action type\n",
    "\n",
    "                metadata = cleaned_candidate_node.get(\"element_metadata\")\n",
    "                if isinstance(metadata, dict):\n",
    "                    identifiers = metadata.get(\"identifiers\")\n",
    "                    if isinstance(identifiers, dict):\n",
    "                        # Prefer CSS selector, fall back to XPath\n",
    "                        ui_selector = identifiers.get(\"selector\")\n",
    "                        identifier = ui_selector # Use selector for both by default\n",
    "                        if not ui_selector:\n",
    "                            xpath_selector = identifiers.get(\"xpath\")\n",
    "                            if xpath_selector:\n",
    "                                ui_selector = f\"xpath={xpath_selector}\" # Prefix XPath\n",
    "                                identifier = ui_selector\n",
    "\n",
    "                # --- Build highlighting and validation lists ---\n",
    "                highlighting_list = []\n",
    "                if ui_selector:\n",
    "                    highlighting_list.append({\"ui_selector\": ui_selector})\n",
    "\n",
    "                completeness_validation_list = []\n",
    "                if identifier and action_type != \"unknown\":\n",
    "                     # Only add validation if we have an identifier and a known action\n",
    "                    completeness_validation_list.append({\n",
    "                        \"action_type\": action_type,\n",
    "                        \"identifier\": identifier\n",
    "                    })\n",
    "\n",
    "                # --- Create the step object ---\n",
    "                step = {\n",
    "                    \"id\": step_id,\n",
    "                    \"text\": step_text,\n",
    "                    \"friendly_text\": step_friendly_text,\n",
    "                    \"audio_id\": step_audio_id, # Set as requested\n",
    "                    \"experience_mapping\": {\n",
    "                        \"highlighting\": highlighting_list,\n",
    "                        \"completeness_validation\": completeness_validation_list\n",
    "                    }\n",
    "                }\n",
    "                guided_experience[\"steps\"].append(step)\n",
    "\n",
    "        # Add the completed guided experience for this sample\n",
    "        guided_experiences[sample_id] = guided_experience\n",
    "\n",
    "\n",
    "    if not guided_experiences:\n",
    "         return {\"error\": f\"No successful reruns found to generate guided experience for workflow '{workflow_id}'.\"}\n",
    "\n",
    "    # If you only want the guide for the *first* successful sample:\n",
    "    # first_sample_id = next(iter(guided_experiences))\n",
    "    # return guided_experiences[first_sample_id]\n",
    "\n",
    "    # If you want guides for *all* successful samples:\n",
    "    return guided_experiences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis stats saved to: clients\\intuit\\add_cust_v98\\analysis_add_cust_v98.json\n",
      "\n",
      "--- Generated Guided Experience JSON ---\n",
      "{\n",
      "  \"id\": \"add_cust_v98-guide-0-0\",\n",
      "  \"description\": \"To add a new customer in QuickBooks Online, go to the Sales tab, then select Customers, and click New customer. Fill out the customer\\u2019s details in the form and hit Save to finish.\\n\",\n",
      "  \"topics\": [],\n",
      "  \"few_shot_examples\": [],\n",
      "  \"intent\": \"add_cust_v98\",\n",
      "  \"steps\": [\n",
      "    {\n",
      "      \"id\": \"add_cust_v98-guide-0-0-step-0\",\n",
      "      \"text\": \"Click on the Sales tab in the left navigation menu\",\n",
      "      \"friendly_text\": \"The 'Sales' menu item is located in the left navigation panel, approximately 1/3 of the way down the menu. It has a right-facing arrow indicating it can be expanded. It's positioned below 'Transactions' and above 'Expenses' in the menu hierarchy.\",\n",
      "      \"audio_id\": \"add_cust_v98-guide-0-0-step-0\",\n",
      "      \"experience_mapping\": {\n",
      "        \"highlighting\": [\n",
      "          {\n",
      "            \"ui_selector\": \".NavItem__NavLabel-sc-494hzp-2.kvWDdZ\"\n",
      "          }\n",
      "        ],\n",
      "        \"completeness_validation\": [\n",
      "          {\n",
      "            \"action_type\": \"click\",\n",
      "            \"identifier\": \".NavItem__NavLabel-sc-494hzp-2.kvWDdZ\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"add_cust_v98-guide-0-0-step-0\",\n",
      "      \"text\": \"Click the 'New customer' button to open the new customer form\",\n",
      "      \"friendly_text\": \"A green rectangular button labeled 'New customer' positioned in the top right area of the page. It has white text and is approximately 150 pixels wide. It's located at coordinates x: 960, y: 200, to the right of the 'Customer types' button and below the main navigation tabs.\",\n",
      "      \"audio_id\": \"add_cust_v98-guide-0-0-step-0\",\n",
      "      \"experience_mapping\": {\n",
      "        \"highlighting\": [\n",
      "          {\n",
      "            \"ui_selector\": \".Button-label-71e8ba3\"\n",
      "          }\n",
      "        ],\n",
      "        \"completeness_validation\": [\n",
      "          {\n",
      "            \"action_type\": \"click\",\n",
      "            \"identifier\": \".Button-label-71e8ba3\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"add_cust_v98-guide-0-0-step-0\",\n",
      "      \"text\": \"Enter a value in the 'Customer display name' field which is required (marked with an asterisk)\",\n",
      "      \"friendly_text\": \"The 'Customer display name' input field is a text box located in the upper portion of the form, to the right of the label 'Customer display name *'. It's positioned below the 'Company name' field and to the right of it. The field is approximately in the middle-right area of the form.\",\n",
      "      \"audio_id\": \"add_cust_v98-guide-0-0-step-0\",\n",
      "      \"experience_mapping\": {\n",
      "        \"highlighting\": [\n",
      "          {\n",
      "            \"ui_selector\": \"#idsDropdownTypeaheadTextField2\"\n",
      "          }\n",
      "        ],\n",
      "        \"completeness_validation\": [\n",
      "          {\n",
      "            \"action_type\": \"click\",\n",
      "            \"identifier\": \"#idsDropdownTypeaheadTextField2\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"add_cust_v98-guide-0-0-step-1\",\n",
      "      \"text\": \"Type a customer display name in the field\",\n",
      "      \"friendly_text\": \"The now-focused 'Customer display name' input field where text will be entered\",\n",
      "      \"audio_id\": \"add_cust_v98-guide-0-0-step-1\",\n",
      "      \"experience_mapping\": {\n",
      "        \"highlighting\": [\n",
      "          {\n",
      "            \"ui_selector\": \"#idsDropdownTypeaheadTextField2\"\n",
      "          }\n",
      "        ],\n",
      "        \"completeness_validation\": [\n",
      "          {\n",
      "            \"action_type\": \"type\",\n",
      "            \"identifier\": \"#idsDropdownTypeaheadTextField2\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"add_cust_v98-guide-0-0-step-2\",\n",
      "      \"text\": \"Enter an email address in the Email field\",\n",
      "      \"friendly_text\": \"The 'Email' input field is a text box located in the middle portion of the form, below the customer name fields. It's positioned to the left of the 'Phone number' field.\",\n",
      "      \"audio_id\": \"add_cust_v98-guide-0-0-step-2\",\n",
      "      \"experience_mapping\": {\n",
      "        \"highlighting\": [\n",
      "          {\n",
      "            \"ui_selector\": \"#idsTxtField221\"\n",
      "          }\n",
      "        ],\n",
      "        \"completeness_validation\": [\n",
      "          {\n",
      "            \"action_type\": \"click\",\n",
      "            \"identifier\": \"#idsTxtField221\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"add_cust_v98-guide-0-0-step-3\",\n",
      "      \"text\": \"Type an email address in the Email field\",\n",
      "      \"friendly_text\": \"The now-focused 'Email' input field where text will be entered\",\n",
      "      \"audio_id\": \"add_cust_v98-guide-0-0-step-3\",\n",
      "      \"experience_mapping\": {\n",
      "        \"highlighting\": [\n",
      "          {\n",
      "            \"ui_selector\": \"#idsTxtField221\"\n",
      "          }\n",
      "        ],\n",
      "        \"completeness_validation\": [\n",
      "          {\n",
      "            \"action_type\": \"type\",\n",
      "            \"identifier\": \"#idsTxtField221\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"add_cust_v98-guide-0-0-step-4\",\n",
      "      \"text\": \"Click the Save button to create the customer\",\n",
      "      \"friendly_text\": \"The 'Save' button is a green button located at the bottom right corner of the form. It has white text that says 'Save'.\",\n",
      "      \"audio_id\": \"add_cust_v98-guide-0-0-step-4\",\n",
      "      \"experience_mapping\": {\n",
      "        \"highlighting\": [\n",
      "          {\n",
      "            \"ui_selector\": \".Button-label-71e8ba3\"\n",
      "          }\n",
      "        ],\n",
      "        \"completeness_validation\": [\n",
      "          {\n",
      "            \"action_type\": \"click\",\n",
      "            \"identifier\": \".Button-label-71e8ba3\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Guided experience saved to: clients\\intuit\\add_cust_v98\\guide_add_cust_v98_sample_0.json\n",
      "\n",
      "--- Field Mapping Summary ---\n",
      "- output.id: Generated based on workflow, sample, and successful rerun ID (e.g., 'add_cust_v98-guide-0-0')\n",
      "- output.description: Taken from original_workflow_data['workflow_instructions']\n",
      "- output.topics: Set to [] (empty list)\n",
      "- output.few_shot_examples: Set to [] (empty list)\n",
      "- output.intent: Taken from analysis_results['workflow_id']\n",
      "- output.steps: Generated list based on successful actions in the cleaned path\n",
      "  - step.id: Generated based on experience_id and action_id (e.g., 'add_cust_v98-guide-0-0-step-0')\n",
      "  - step.text: Taken from cleaned_action_node['description']\n",
      "  - step.friendly_text: Taken from cleaned_candidate_node['element_description']\n",
      "  - step.audio_id: Set to the same value as step.id\n",
      "  - step.experience_mapping.highlighting[0].ui_selector: Taken from cleaned_candidate_node['element_metadata']['identifiers']['selector'] (fallback to XPath if selector is missing)\n",
      "  - step.experience_mapping.completeness_validation[0].action_type: Taken from cleaned_candidate_node['action']\n",
      "  - step.experience_mapping.completeness_validation[0].identifier: Taken from cleaned_candidate_node['element_metadata']['identifiers']['selector'] (fallback to XPath if selector is missing)\n",
      "\n",
      "--- Skipped/Blank Fields ---\n",
      "- output.topics: Intentionally set to an empty list as requested.\n",
      "- output.few_shot_examples: Intentionally set to an empty list as requested.\n",
      "- step.audio_id: Set to step.id as a placeholder; no actual audio mapping performed.\n",
      "- Highlighting/Validation: These lists will be empty if the necessary 'element_metadata' or 'identifiers' were missing in the source JSON for a specific step.\n",
      "- Other metadata: Fields like 'created_at', 'open_log', specific run/retry details (beyond the successful path), error messages, advice, etc., from the original logger.json are *not* included in the final guided experience JSON structure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dusan\\AppData\\Local\\Temp\\ipykernel_5456\\1066280102.py:74: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"analysis_timestamp\": datetime.datetime.utcnow().isoformat(),\n"
     ]
    }
   ],
   "source": [
    "client = \"intuit\"\n",
    "\n",
    "workflow_to_analyze = \"add_cust_v98\" # Use the workflow ID you want\n",
    "\n",
    "\n",
    "json_file = Path(f\"clients/{client}/logger.json\") # Make sure this path is correct\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load original data to get workflow instructions\n",
    "original_data = None\n",
    "try:\n",
    "    with open(json_file, 'r') as f:\n",
    "        original_data = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading original data: {e}\")\n",
    "    exit()\n",
    "\n",
    "if not original_data or workflow_to_analyze not in original_data.get(\"workflows\", {}):\n",
    "    print(f\"Workflow '{workflow_to_analyze}' not found in original data.\")\n",
    "    exit()\n",
    "\n",
    "original_workflow_data_for_analysis = original_data[\"workflows\"][workflow_to_analyze]\n",
    "\n",
    "# 2. Run the analysis function from the previous step\n",
    "# Make sure the analyze_workflow_logs function is defined or imported\n",
    "try:\n",
    "    # Assuming analyze_workflow_logs is defined in this file or imported\n",
    "    analysis = analyze_workflow_logs(json_file, workflow_to_analyze)\n",
    "    if \"error\" in analysis:\n",
    "            print(f\"Analysis Error: {analysis['error']}\")\n",
    "            exit()\n",
    "\n",
    "    analysis_file = Path(f\"clients/{client}/{workflow_to_analyze}/analysis_{workflow_to_analyze}.json\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure the directory exists and remains accessible\n",
    "        analysis_file.parent.mkdir(parents=True, exist_ok=True,mode=0o755)\n",
    "        with open(analysis_file, 'w') as f:\n",
    "            json.dump(analysis, f, indent=2) # Save the analysis results\n",
    "        print(f\"\\nAnalysis stats saved to: {analysis_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving analysis stats file: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "except NameError:\n",
    "    print(\"Error: analyze_workflow_logs function not found. Make sure it's defined or imported.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error during analysis: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# 3. Transform the analysis results\n",
    "guided_experience_output = transform_to_guided_experience(\n",
    "    analysis,\n",
    "    original_workflow_data_for_analysis\n",
    ")\n",
    "\n",
    "# 4. Print or save the result\n",
    "if guided_experience_output and \"error\" not in guided_experience_output:\n",
    "    print(\"\\n--- Generated Guided Experience JSON ---\")\n",
    "    # Print the guide for the first sample found (adjust if you want all)\n",
    "    first_sample_key = next(iter(guided_experience_output))\n",
    "    print(json.dumps(guided_experience_output[first_sample_key], indent=2))\n",
    "\n",
    "    # Example: Save the first guide to a new file\n",
    "    guide_file = Path(f\"clients/{client}/{workflow_id}/guide_{workflow_to_analyze}_sample_{first_sample_key}.json\")\n",
    "    try:\n",
    "        with open(guide_file, 'w') as f:\n",
    "                json.dump(guided_experience_output[first_sample_key], f, indent=2)\n",
    "        print(f\"\\nGuided experience saved to: {guide_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving guided experience file: {e}\")\n",
    "elif guided_experience_output and \"error\" in guided_experience_output:\n",
    "        print(f\"\\nTransformation Error: {guided_experience_output['error']}\")\n",
    "else:\n",
    "    print(\"\\nNo guided experience generated (likely no successful reruns found).\")\n",
    "\n",
    "\n",
    "# --- Mapping Summary & Skipped Fields ---\n",
    "\n",
    "print(\"\\n--- Field Mapping Summary ---\")\n",
    "print(f\"- output.id: Generated based on workflow, sample, and successful rerun ID (e.g., '{workflow_to_analyze}-guide-0-0')\")\n",
    "print(f\"- output.description: Taken from original_workflow_data['workflow_instructions']\")\n",
    "print(f\"- output.topics: Set to [] (empty list)\")\n",
    "print(f\"- output.few_shot_examples: Set to [] (empty list)\")\n",
    "print(f\"- output.intent: Taken from analysis_results['workflow_id']\")\n",
    "print(f\"- output.steps: Generated list based on successful actions in the cleaned path\")\n",
    "print(f\"  - step.id: Generated based on experience_id and action_id (e.g., '{workflow_to_analyze}-guide-0-0-step-0')\")\n",
    "print(f\"  - step.text: Taken from cleaned_action_node['description']\")\n",
    "print(f\"  - step.friendly_text: Taken from cleaned_candidate_node['element_description']\")\n",
    "print(f\"  - step.audio_id: Set to the same value as step.id\")\n",
    "print(f\"  - step.experience_mapping.highlighting[0].ui_selector: Taken from cleaned_candidate_node['element_metadata']['identifiers']['selector'] (fallback to XPath if selector is missing)\")\n",
    "print(f\"  - step.experience_mapping.completeness_validation[0].action_type: Taken from cleaned_candidate_node['action']\")\n",
    "print(f\"  - step.experience_mapping.completeness_validation[0].identifier: Taken from cleaned_candidate_node['element_metadata']['identifiers']['selector'] (fallback to XPath if selector is missing)\")\n",
    "\n",
    "print(\"\\n--- Skipped/Blank Fields ---\")\n",
    "print(\"- output.topics: Intentionally set to an empty list as requested.\")\n",
    "print(\"- output.few_shot_examples: Intentionally set to an empty list as requested.\")\n",
    "print(\"- step.audio_id: Set to step.id as a placeholder; no actual audio mapping performed.\")\n",
    "print(\"- Highlighting/Validation: These lists will be empty if the necessary 'element_metadata' or 'identifiers' were missing in the source JSON for a specific step.\")\n",
    "print(\"- Other metadata: Fields like 'created_at', 'open_log', specific run/retry details (beyond the successful path), error messages, advice, etc., from the original logger.json are *not* included in the final guided experience JSON structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
